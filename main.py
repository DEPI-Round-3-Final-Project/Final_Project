import os
from config import TELEGRAM_TOKEN, PDF_DIRECTORY, EXTRACTED_TEXT_DIRECTORY
from data_extractor import PDFExtractor
from text_preprocessor import TextPreprocessor
from database_manager import DatabaseManager
from telegram_bot import StudyAssistantBot
from reminder_system import ReminderSystem


def setup_directories():
    """Create directories"""
    os.makedirs(PDF_DIRECTORY, exist_ok=True)
    os.makedirs(EXTRACTED_TEXT_DIRECTORY, exist_ok=True)
    os.makedirs("rag_cache", exist_ok=True)  # ğŸ†• cache folder
    print("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª\n")


def process_single_pdf(pdf_name, subject_name):
    """Process only one book (Biology or Arabic)"""
    pdf_path = os.path.join(PDF_DIRECTORY, pdf_name)

    if not os.path.exists(pdf_path):
        print(f"âŒ ØªØ­Ø°ÙŠØ±: Ù…Ù„Ù {pdf_name} ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯!\n")
        return

    print("\n" + "="*70)
    print(f"ğŸ“š Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒØªØ§Ø¨: {subject_name}")
    print("="*70 + "\n")

    preprocessor = TextPreprocessor()
    db_manager = DatabaseManager()
    extractor = PDFExtractor()

    processed_chunks = 0
    total_characters = 0

    for page_text, page_num in extractor.process_pdf_page_by_page(pdf_path):

        # Clean the text
        processed_text = preprocessor.preprocess_text(page_text)

        if not preprocessor.is_meaningful_text(processed_text):
            continue

        # Split the text into chunks
        for chunk in preprocessor.chunk_text(processed_text, chunk_size=800, overlap=100):

            if len(chunk) > 100:  # Ignore very short texts

                # Divide the chapter based on every 20 pages
                chapter_num = (page_num - 1) // 20 + 1

                db_manager.add_textbook_content(
                    subject=subject_name,
                    grade_level="secondary",
                    chapter=f"Chapter {chapter_num}",
                    content=chunk,
                    page_number=page_num,
                    content_type="text"
                )

                processed_chunks += 1
                total_characters += len(chunk)

    print("\n" + "="*70)
    print(f"âœ… Ø§ÙƒØªÙ…Ù„Øª Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒØªØ§Ø¨: {subject_name}")
    print(f"ğŸ“Š Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª:")
    print(f" Â  â€¢ Ø¹Ø¯Ø¯ Ø§Ù„Ù‚Ø·Ø¹: {processed_chunks}")
    print(f" Â  â€¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ø­Ø±Ù: {total_characters:,}")
    print(
        f" Â  â€¢ Ù…ØªÙˆØ³Ø· Ø·ÙˆÙ„ Ø§Ù„Ù‚Ø·Ø¹Ø©: {total_characters // max(processed_chunks, 1)} Ø­Ø±Ù")
    print("="*70 + "\n")


def process_pdfs():
    """Process all PDF files (Biology + Arabic)"""
    # Process Biology
    process_single_pdf("biology.pdf", "biology")

    # Process Arabic
    process_single_pdf("arabic.pdf", "arabic")


def verify_database():
    """Verify the database"""
    print("\n" + "="*70)
    print("ğŸ” Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
    print("="*70 + "\n")

    db_manager = DatabaseManager()

    for subject in ["biology", "arabic"]:
        content = db_manager.get_textbook_content(subject)

        if content:
            print(f"âœ… Ù…Ø­ØªÙˆÙ‰ {subject}: {len(content)} Ù‚Ø·Ø¹Ø©")

            # Show sample
            sample = content[0]
            print(f"\nğŸ“ Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ({subject}):")
            print(f" Â  Ø§Ù„ÙØµÙ„: {sample[0]}")
            print(f" Â  Ø§Ù„ØµÙØ­Ø©: {sample[2]}")
            preview = sample[1][:150].replace('\n', ' ')
            print(f" Â  Ø§Ù„Ù†Øµ: {preview}...\n")
        else:
            print(f"âš ï¸ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ø­ØªÙˆÙ‰ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù…Ø§Ø¯Ø© {subject}!\n")

    print("="*70 + "\n")


def check_cache_status():
    """ğŸ†• Check cache status"""
    cache_files = [
        "rag_cache/faiss_index.bin",
        "rag_cache/texts.pkl",
        "rag_cache/metadata.pkl",
        "rag_cache/embeddings_cache.pkl"
    ]

    all_exist = all(os.path.exists(f) for f in cache_files)

    if all_exist:
        print("\n" + "="*70)
        print("ğŸ’¾ Ø­Ø§Ù„Ø© Ø§Ù„Ù€ Cache:")
        print("="*70)
        print("âœ… ÙˆØ¬Ø¯Øª Ù…Ù„ÙØ§Øª cache Ù…Ø­ÙÙˆØ¸Ø©!")
        print("âš¡ Ø³ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙ‡Ø±Ø³ Ù…Ø¨Ø§Ø´Ø±Ø© (Ø¨Ø¯ÙˆÙ† Ø¥Ø¹Ø§Ø¯Ø© Ù…Ø¹Ø§Ù„Ø¬Ø©)")

        # Show file sizes
        for f in cache_files:
            size = os.path.getsize(f) / (1024 * 1024)  # MB
            print(f" Â  ğŸ“ {os.path.basename(f)}: {size:.2f} MB")

        print("="*70 + "\n")
        return True
    else:
        print("\n" + "="*70)
        print("ğŸ“ Ø­Ø§Ù„Ø© Ø§Ù„Ù€ Cache:")
        print("="*70)
        print("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ cache Ù…Ø­ÙÙˆØ¸")
        print("â³ Ø³ÙŠØªÙ… Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ (Ø³ÙŠØ³ØªØºØ±Ù‚ Ø¨Ø¹Ø¶ Ø§Ù„ÙˆÙ‚Øª ÙÙŠ Ø£ÙˆÙ„ Ù…Ø±Ø©)")
        print("="*70 + "\n")
        return False


def main():
    """Main function - updated and enhanced"""
    print("\n" + "="*70)
    print("ğŸ¤– Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø¯Ø±Ø§Ø³Ø© Ø§Ù„Ø°ÙƒÙŠ - Smart Study Assistant")
    print("ğŸ“¦ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø© v3.0")
    print("="*70 + "\n")

    # Create directories
    setup_directories()

    # ğŸ†• Check cache status
    cache_exists = check_cache_status()

    # Check for data existence
    db_manager = DatabaseManager()
    biology_content = db_manager.get_textbook_content("biology")
    arabic_content = db_manager.get_textbook_content("arabic")

    # If the database is empty, process PDF
    if not biology_content and not arabic_content:
        print("ğŸ“š Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙØ§Ø±ØºØ©ØŒ Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØªØ¨...\n")
        process_pdfs()
        verify_database()
    else:
        print(f"âœ… Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø§Ù‡Ø²Ø©!")
        print(f" Â  â€¢ Ø§Ù„Ø£Ø­ÙŠØ§Ø¡: {len(biology_content)} Ù‚Ø·Ø¹Ø©")
        print(f" Â  â€¢ Ø§Ù„Ø¹Ø±Ø¨ÙŠ: {len(arabic_content)} Ù‚Ø·Ø¹Ø©\n")

    # Run the bot
    print("="*70)
    print("ğŸš€ ØªØ´ØºÙŠÙ„ Telegram Bot...")
    print("="*70 + "\n")

    bot = StudyAssistantBot(TELEGRAM_TOKEN)

    # Run the reminder system
    print("â° ØªØ´ØºÙŠÙ„ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ°ÙƒÙŠØ±Ø§Øª...")
    reminder_system = ReminderSystem(bot.updater.bot, db_manager)
    reminder_system.start()
    print("âœ… Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ°ÙƒÙŠØ±Ø§Øª ÙŠØ¹Ù…Ù„!\n")

    print("="*70)
    print("âœ… Ø§Ù„Ø¨ÙˆØª ÙŠØ¹Ù…Ù„ Ø§Ù„Ø¢Ù† Ø¨ÙƒØ§Ù…Ù„ Ø§Ù„Ù…ÙŠØ²Ø§Øª!")
    print("="*70)
    print("\nğŸ¯ **Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©:**")
    print(" Â  1. âœ… ØªØ­Ø³ÙŠÙ† Ø¯Ù‚Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª")
    print(" Â  2. ğŸ¨ ÙˆØ§Ø¬Ù‡Ø© Ù…Ø³ØªØ®Ø¯Ù… Ù…Ø­Ø³Ù‘Ù†Ø©")
    print(" Â  3. â° Ù†Ø¸Ø§Ù… ØªØ°ÙƒÙŠØ±Ø§Øª Ø°ÙƒÙŠ")
    print(" Â  4. ğŸ“‹ Ø¥Ø¯Ø§Ø±Ø© Ù…Ù‡Ø§Ù… Ù…ØªÙ‚Ø¯Ù…Ø©")
    print(" Â  5. ğŸ¯ Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª ØªÙØ§Ø¹Ù„ÙŠØ©")
    print(" Â  6. ğŸ” ÙÙ„ØªØ±Ø© Ø°ÙƒÙŠØ© Ù„Ù„Ù…ÙˆØ§Ø¯")
    print(" Â  7. ğŸ’¾ Ø­ÙØ¸ Ø§Ù„Ù€ cache (Ù„Ø§ Ø¥Ø¹Ø§Ø¯Ø© Ù…Ø¹Ø§Ù„Ø¬Ø©)")
    print(" Â  8. ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¯Ù‚ÙŠÙ‚Ø© Ù„Ù„Ù…Ù‡Ø§Ù…\n")

    if cache_exists:
        print("ğŸ’¡ ØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù€ cache Ø§Ù„Ù…Ø­ÙÙˆØ¸ - Ø§Ù„Ø¨ÙˆØª Ø¬Ø§Ù‡Ø² ÙÙˆØ±Ø§Ù‹!\n")
    else:
        print("ğŸ’¡ ØªÙ… Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ ÙˆØ­ÙØ¸Ù‡ - Ø§Ù„Ù…Ø±Ø© Ø§Ù„Ù‚Ø§Ø¯Ù…Ø© Ø³ØªÙƒÙˆÙ† Ø£Ø³Ø±Ø¹!\n")

    try:
        bot.run()
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸ Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø¨ÙˆØª...")
        reminder_system.stop()
        print("âœ… ØªÙ… Ø§Ù„Ø¥ÙŠÙ‚Ø§Ù Ø¨Ù†Ø¬Ø§Ø­")


if __name__ == "__main__":
    main()